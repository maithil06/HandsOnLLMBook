{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On Large Language Models Language Understanding and Generation"
      ],
      "metadata": {
        "id": "yNI0aOAMSraK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 7\n"
      ],
      "metadata": {
        "id": "9FzumjXlTYGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ],
      "metadata": {
        "id": "i7NbgpnVTa77",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama.cpp.python"
      ],
      "metadata": {
        "id": "eHR0C1cmUNq_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "pg84Rd_-UWzI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c46cc792",
        "collapsed": true
      },
      "source": [
        "!pip install langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(\n",
        "model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "n_gpu_layers=-1,\n",
        "max_tokens=500,\n",
        "n_ctx=2048,\n",
        "seed=42,\n",
        "verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "tnQga8izckvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2b4a57-0d95-4aea-bd45-71431b02cd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
      ],
      "metadata": {
        "id": "lXsPhL3lTqIx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70113f91-ce82-4e61-bf73-4f28faf72592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "template = \"\"\"<s><|user|> {input_prompt} <|end|> <|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(template = template, input_variables = [\"input_prompt\"])"
      ],
      "metadata": {
        "id": "76fd2onET1LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain = prompt | llm"
      ],
      "metadata": {
        "id": "8jW5ccJD61Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain.invoke({\"input_prompt\" : \"Hi!, my name is Marteen. What is 1+1?\"})"
      ],
      "metadata": {
        "id": "qFSRY29U67nA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "29a78713-819f-4f2c-895a-054cc9db4120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Marteen! The answer to 1+1 is 2. It's a basic arithmetic addition where one plus another one equals two.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Create a funny name for a business that sells {product}.\"\n",
        "name_prompt = PromptTemplate( template=template, input_variables=[\"product\"])"
      ],
      "metadata": {
        "id": "upIwIdNW7wKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall langchain langchain-community"
      ],
      "metadata": {
        "id": "fZUyfrwg1Vaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import LLMChain\n",
        "\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
        "title.invoke({\"summary\": \"a girl that lost her mother\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAgdYu-BxGsn",
        "outputId": "aeda9e7d-c33b-482b-c15b-ecc9966f41c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3143042960.py:7: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of a Farewell: The Journey Through Loss\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}.\n",
        "Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ],
      "metadata": {
        "id": "Xey1q_tfwYxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
        "\n",
        "title.invoke({\"summary\": \"a girl that lost her mother\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Z3oYgWwuf8",
        "outputId": "3a6db7a6-fc0d-41c3-c245-e1333862511c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Lily\\'s Lament: A Journey Through Grief\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main character is:\n",
        "{character}. Only return the story and it cannot be longer than one paragraph.\n",
        "<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ],
      "metadata": {
        "id": "8LDN1wN2wx2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = title | character | story"
      ],
      "metadata": {
        "id": "3iL08Issw1w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"a girl that lost her mother\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkEMVlm1w5Rh",
        "outputId": "0cab23f6-6aca-481a-dc23-70f2da829b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Fading Echoes: A Tale of a Motherless Girl\\'s Journey\"',\n",
              " 'character': \" The main character, Emily, is a resilient and introspective young girl who has grown up without her mother's nurturing presence. As she navigates the challenges of life without maternal guidance, she learns to rely on inner strength, determination, and newfound friendships to fill the void left by her mother's passing.\",\n",
              " 'story': \" Fading Echoes: A Tale of a Motherless Girl's Journey follows Emily, an introspective and resilient young girl whose mother passed away when she was just five years old. Despite the void left by her mother's absence, Emily discovers strength within herself as she navigates life without maternal guidance. With every challenge that comes her way, Emily learns to lean on her inner resolve and determination, forging new friendships along her journey that fill the gaps of lost love. Through heartwarming adventures, poignant memories, and personal growth, Emily's tale serves as a testament to the power of resilience in overcoming life's obstacles and finding solace in the echoes of cherished moments shared with her mother.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IruZvWYAxeKI",
        "outputId": "295e6b52-80af-400a-9687-36f16090361e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Maarten! The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iQOjsyP6xgWd",
        "outputId": "ac3310b0-b4e1-45f4-cc77-0b45f46755c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" As an AI, I'm unable to determine your name without any context or information provided. If you would like to share your name for this conversation, please do so! Remember that privacy is important, and sharing personal details should be done with trusted entities when necessary.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate( template=template,\n",
        "input_variables=[\"input_prompt\", \"chat_history\"])"
      ],
      "metadata": {
        "id": "CKC2kJ-Ixk5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "prompt=prompt,\n",
        "llm=llm,\n",
        "memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "OV7noQ2yxqXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AzXxICpxvFs",
        "outputId": "b9e4beb0-82d2-4e0f-9b18-001163c1d28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether.\\n\\nHere's a simple explanation:\\nImagine you have one apple and then someone gives you another apple. Now you have a total of two apples. This concept applies to numbers as well; when you add 1 (one) with another 1 (one), you get 2 (two).\"}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZqnLj3Mxx98",
        "outputId": "bb31958b-782e-4ca5-b438-bac0d8e94ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether.\\n\\nHere's a simple explanation:\\nImagine you have one apple and then someone gives you another apple. Now you have a total of two apples. This concept applies to numbers as well; when you add 1 (one) with another 1 (one), you get 2 (two).\",\n",
              " 'text': ' Hi Maarten! My name is Assistant. Nice to meet you.\\nYou were actually asking for the answer to 1 + 1, which is indeed 2 as I explained earlier.'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "llm_chain = LLMChain(\n",
        "prompt=prompt,\n",
        "llm=llm,\n",
        "memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUXTuqmFx45i",
        "outputId": "e058f4dd-cb01-4438-fc4c-cb62f4db48a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2813140627.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(input_prompt=\"Hi! My name is Maarten and I am 33 years old.What is 1 + 1?\")\n",
        "llm_chain.predict(input_prompt=\"What is 3 + 3?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Htwm-HAdx9gV",
        "outputId": "02608599-b6d5-45eb-adf5-9cefd55aac5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi Maarten! Given the nature of your new question, which is basic arithmetic as well, I'll provide you with the answer: 3 + 3 equals 6. If there's anything else on your mind or if you have more questions about different topics, feel free to ask. Enjoy the rest of your day! üòä\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfGHYM_hyFrI",
        "outputId": "a3227b76-9a02-4b5e-fe9b-828e6a76d81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old.What is 1 + 1?\\nAI:  Hello Maarten! Given the nature of your question, which is basic arithmetic, I'll provide you with the answer: 1 + 1 equals 2. It looks like we started off with a simple math problem amidst introductions, but it's always good to keep things light and engaging in conversation! If there's anything else on your mind or if you have more questions about different topics, feel free to ask. Enjoy the rest of your day! üòä\\nHuman: What is 3 + 3?\\nAI:  Hi Maarten! Given the nature of your new question, which is basic arithmetic as well, I'll provide you with the answer: 3 + 3 equals 6. If there's anything else on your mind or if you have more questions about different topics, feel free to ask. Enjoy the rest of your day! üòä\",\n",
              " 'text': \" Hello! You've introduced yourself as Maarten, so your name is Maarten. If there's anything else you'd like to discuss or inquire about, feel free to let me know. I'm here to help! üòä As for the arithmetic question, 3 + 3 indeed equals 6. Have a great day ahead! üëç\"}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBMD8xNf0ljN",
        "outputId": "f1144ba6-070d-4ebd-d383-6e68972f5c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 3 + 3?\\nAI:  Hi Maarten! Given the nature of your new question, which is basic arithmetic as well, I'll provide you with the answer: 3 + 3 equals 6. If there's anything else on your mind or if you have more questions about different topics, feel free to ask. Enjoy the rest of your day! üòä\\nHuman: What is my name?\\nAI:  Hello! You've introduced yourself as Maarten, so your name is Maarten. If there's anything else you'd like to discuss or inquire about, feel free to let me know. I'm here to help! üòä As for the arithmetic question, 3 + 3 indeed equals 6. Have a great day ahead! üëç\",\n",
              " 'text': \" As an AI, I'm unable to determine your age without specific information. Age is considered personal data and privacy-sensitive information. However, if you need assistance with calculating age based on a given date or event, feel free to ask!\\n\\nBut remember, it's essential to respect user privacy when using AI platforms by not sharing sensitive information like your name, age, etc., unless the platform explicitly allows such data collection for specific purposes.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update\n",
        "with the new lines.\n",
        "Current summary:\n",
        "{summary}\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "input_variables=[\"new_lines\", \"summary\"],\n",
        "template=summary_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "wVsdakxY0qnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "llm=llm,\n",
        "memory_key=\"chat_history\",\n",
        "prompt=summary_prompt\n",
        ")\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "prompt=prompt,\n",
        "llm=llm,\n",
        "memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHY-Kt1-12Qb",
        "outputId": "e774dd51-e4ab-41bf-cfc1-ca8a39485f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3844021763.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbvY-1EN15w-",
        "outputId": "7e4ef579-1ee5-4010-eca5-f8c1500b3eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Only run this only if you have openAi api key"
      ],
      "metadata": {
        "id": "EXGxtIaH4Dp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "# Load OpenAI's LLMs with LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "HmLFOot93Mb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "react_template = \"\"\"Answer the following questions as best you can. You have\n",
        "access to the following tools:\n",
        "{tools}\n",
        "Use the following format:\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "Begin!\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "template=react_template,\n",
        "input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Hw8aOsMB3jMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "name=\"duckduck\",\n",
        "description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "func=search.run,\n",
        ")\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "tools.append(search_tool)"
      ],
      "metadata": {
        "id": "p4Cfua_Z3pTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "0qafSs6P3nOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"})"
      ],
      "metadata": {
        "id": "9kwAlZXi3wr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3fdL1am35f0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}