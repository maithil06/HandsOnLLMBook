# Hands-On Large Language Models  
### Learning and Code Companion

This repository documents my **end-to-end learning journey through the book**  
**_Hands-On Large Language Models: Language Understanding and Generation_ (Oâ€™Reilly)**  
with full code implementations, experiments, notes, and practical extensions.

This is not just a reproduction of the bookâ€™s material.  
It is a hands-on engineering project focused on:

- deep understanding of modern LLM systems  
- building everything from scratch  
- experimenting beyond textbook examples  
- creating reusable components for real-world applications  

---

## Learning Coverage

### Part I: Understanding Language Models

| Chapter | Topic | Status |
|--------|------|--------|
| 1 | Introduction to Large Language Models | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1AQs5R8om93AFp7v-EyZZxTtqO_eGk8Lg?usp=sharing) |
| 2 | Tokens and Embeddings | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1LUxvelvWzG1EyrUaBmK3CA1QV0vWU4Qj?usp=sharing) |
| 3 | Attention and Transformer Architectures | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wIHU_q9CiUDSsRjbuL2bgpCd-F0SMBLy?usp=sharing) |
| 4 | Encoder-Only and Decoder-Only Models | Yet to be published |
| 5 | Training Paradigms of LLMs | Yet to be published |
| 6 | Responsible LLM Development | Yet to be published |

---

### Part II: Using Pretrained Language Models

| Chapter | Topic | Status |
|--------|------|--------|
| 7 | Text Classification | Yet to be published |
| 8 | Clustering and Topic Modeling | Yet to be published |
| 9 | Prompt Engineering | Yet to be published |
| 10 | Advanced Text Generation | Yet to be published |
| 11 | Memory and Agent Systems | Yet to be published |
| 12 | Semantic Search and RAG | Yet to be published |
| 13 | Multimodal Language Models | Yet to be published |

---

### Part III: Training and Fine-Tuning Language Models

| Chapter | Topic | Status |
|--------|------|--------|
| 14 | Training Embedding Models | Yet to be published |
| 15 | Contrastive Learning and SBERT | Yet to be published |
| 16 | Fine-Tuning BERT | Yet to be published |
| 17 | Generative Model Fine-Tuning | Yet to be published |
| 18 | PEFT, LoRA and Quantization | Yet to be published |
| 19 | RLHF and Preference Optimization | Yet to be published |

---

## Repository Structure

```text
hands-on-llms/
â”‚
â”œâ”€â”€ 01_understanding_language_models/
â”œâ”€â”€ 02_tokens_and_embeddings/
â”œâ”€â”€ 03_inside_transformers/
â”œâ”€â”€ 04_text_classification/
â”œâ”€â”€ 05_clustering_and_topic_modeling/
â”œâ”€â”€ 06_prompt_engineering/
â”œâ”€â”€ 07_advanced_text_generation/
â”œâ”€â”€ 08_semantic_search_and_rag/
â”œâ”€â”€ 09_multimodal_models/
â”œâ”€â”€ 10_training_and_finetuning/
â”‚
â”œâ”€â”€ datasets/
â”œâ”€â”€ experiments/
â”œâ”€â”€ utils/
â””â”€â”€ README.md
```
## ğŸ› ï¸ Tech Stack

- **Language:** Python  
- **Frameworks:** PyTorch, Hugging Face Transformers, LangChain  
- **Vector Databases:** FAISS, Chroma  
- **LLM APIs:** OpenAI API, Local LLMs  
- **Visualization:** Matplotlib, Seaborn  
- **Tooling:** Jupyter Notebook, VS Code, Git  

---

## ğŸ¯ Project Goals

This repository is designed to be:

- a **complete practical reference for LLM engineering**
- a **hands-on learning log**
- a **portfolio project for ML & LLM roles**
- an **experimentation platform for modern AI systems**

---

## ğŸ“ˆ Roadmap

- [ ] Real-world RAG systems  
- [ ] Multi-agent orchestration  
- [ ] LLM evaluation pipelines  
- [ ] Production deployment workflows  
- [ ] LLMOps monitoring & observability  

---

## ğŸ¤ Contributing

Contributions, feedback, and discussions are welcome.  
Feel free to open issues or submit pull requests.

---

## ğŸ“– Reference

**Hands-On Large Language Models: Language Understanding and Generation**  
_Oâ€™Reilly Media_

This repository is an independent learning companion and is not affiliated with Oâ€™Reilly.


